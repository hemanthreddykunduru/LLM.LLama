# TinyLLaMA: Custom Language Model Training Pipeline

A complete end-to-end pipeline for training a small-scale language model from scratch, including data collection, preprocessing, tokenization, and training with a LLaMA-inspired architecture.

## ğŸŒŸ Features

- **Automated Data Collection**: Scrapes Wikipedia articles and web content
- **Robust Data Preprocessing**: Deduplication, quality filtering, and text cleaning
- **Custom Tokenizer Training**: Uses SentencePiece with BPE encoding
- **Modern Architecture**: Implements LLaMA-inspired features including:
  - RMSNorm for layer normalization
  - Rotary Position Embeddings (RoPE)
  - SwiGLU activation function
  - Multi-head attention with causal masking
- **Training Pipeline**: Complete training loop with validation and checkpointing
- **Inference Engine**: Ready-to-use text generation interface

## ğŸ“‹ Requirements

### Dependencies

```bash
pip install torch torchvision torchaudio
pip install numpy
pip install sentencepiece
pip install wikipedia-api
pip install beautifulsoup4
pip install requests
pip install tqdm
```

### System Requirements

- **GPU**: CUDA-compatible GPU recommended (training will be very slow on CPU)
- **RAM**: At least 8GB RAM, 16GB+ recommended
- **Storage**: 5-10GB free space for data and model checkpoints
- **Python**: 3.8 or higher

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run the Complete Pipeline

```bash
python training.py
```

This will execute all phases:
1. Data collection from Wikipedia
2. Data preprocessing and cleaning
3. Tokenizer training
4. Model training
5. Inference testing

### 3. Monitor Training Progress

The script will show progress bars and training metrics:
- Loss values during training
- Learning rate schedules
- Validation performance
- Checkpoint saving

## ğŸ“Š Pipeline Overview

### Phase 1: Data Collection
- Scrapes Wikipedia articles related to AI topics
- Collects up to 2000 articles per topic
- Supports custom topic lists and web URL scraping
- Saves raw data in JSON format

### Phase 2: Data Preprocessing
- **Exact Deduplication**: Removes identical content using MD5 hashing
- **Near Deduplication**: Uses Jaccard similarity on text shingles
- **Quality Filtering**: Removes low-quality content based on:
  - Text length (100-50,000 characters)
  - Word count and sentence structure
  - Character diversity

### Phase 3: Tokenizer Training
- Trains SentencePiece tokenizer with BPE algorithm
- Vocabulary size: 12,000 tokens
- Includes special tokens: PAD, EOS, UNK, BOS

### Phase 4: Model Architecture

```
TinyLLaMA Model Specifications:
â”œâ”€â”€ Embedding Dimension: 512
â”œâ”€â”€ Number of Layers: 8
â”œâ”€â”€ Attention Heads: 8
â”œâ”€â”€ Vocabulary Size: 12,000
â”œâ”€â”€ Max Sequence Length: 512
â”œâ”€â”€ Parameters: ~15M
â””â”€â”€ Features:
    â”œâ”€â”€ RMSNorm
    â”œâ”€â”€ Rotary Position Embeddings
    â”œâ”€â”€ SwiGLU Activation
    â””â”€â”€ Causal Self-Attention
```

### Phase 5: Training Configuration
- **Optimizer**: AdamW with weight decay (0.1)
- **Learning Rate**: 5e-4 with cosine annealing
- **Batch Size**: 8
- **Epochs**: 100
- **Gradient Clipping**: Max norm 1.0
- **Validation Split**: 90/10 train/validation

## ğŸ”§ Configuration

### Customizing Topics

Edit the `topics` list in `main()`:

```python
topics = [
    "artificial intelligence",
    "machine learning",
    "deep learning",
    "neural networks"
]
```

### Model Hyperparameters

Modify the model configuration:

```python
model = TinyLLaMA(
    vocab_size=tokenizer.vocab_size(),
    dim=512,        # Embedding dimension
    n_layers=8,     # Number of transformer layers
    n_heads=8,      # Number of attention heads
    max_seq_len=512, # Maximum sequence length
    dropout=0.1     # Dropout rate
)
```

### Training Parameters

Adjust training settings:

```python
trainer.train(
    train_data, 
    val_data, 
    epochs=100,      # Number of training epochs
    batch_size=8,    # Batch size
    lr=5e-4         # Learning rate
)
```

## ğŸ“ Output Files

After training, you'll have:

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ wiki_data.json          # Raw scraped data
â”‚   â””â”€â”€ clean_data.json         # Preprocessed data
â”œâ”€â”€ tinyllama_tokenizer.model   # Trained tokenizer
â”œâ”€â”€ tinyllama_tokenizer.vocab   # Tokenizer vocabulary
â”œâ”€â”€ tinyllama_final.pt          # Final trained model
â””â”€â”€ checkpoint_*.pt             # Training checkpoints
```

## ğŸ¯ Usage Examples

### Text Generation

```python
from training import TinyLLaMAInference

# Load trained model
inference = TinyLLaMAInference(
    model_path="tinyllama_final.pt",
    tokenizer_path="tinyllama_tokenizer.model"
)

# Generate text
response = inference.generate_text(
    prompt="The future of artificial intelligence is",
    max_length=100,
    temperature=0.7,
    top_k=50
)

print(response)
```

### Custom Data Training

```python
# Use your own data
custom_data = [
    {"content": "Your text data here...", "source": "custom"},
    {"content": "More text data...", "source": "custom"}
]

# Train with custom data
trainer = Trainer(model, tokenizer, device)
trainer.train(custom_data, val_data, epochs=50)
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

**CUDA Out of Memory**
- Reduce batch size: `batch_size=4` or `batch_size=2`
- Reduce sequence length: `max_seq_len=256`
- Use gradient accumulation

**Slow Training**
- Ensure you're using GPU: Check `torch.cuda.is_available()`
- Reduce model size: `dim=256, n_layers=4`
- Use mixed precision training

**Poor Generation Quality**
- Train for more epochs
- Increase model size
- Improve data quality
- Adjust generation parameters (temperature, top_k)

### Memory Optimization

For limited GPU memory:

```python
# Smaller model configuration
model = TinyLLaMA(
    vocab_size=tokenizer.vocab_size(),
    dim=256,        # Reduced dimension
    n_layers=4,     # Fewer layers
    n_heads=4,      # Fewer heads
    max_seq_len=256, # Shorter sequences
    dropout=0.1
)
```

## ğŸ“ˆ Performance Expectations

### Training Time
- **GPU (RTX 3080)**: ~2-4 hours for 100 epochs
- **GPU (GTX 1060)**: ~8-12 hours for 100 epochs  
- **CPU**: Not recommended (days/weeks)

### Model Quality
- **Small Dataset**: Basic coherence, limited knowledge
- **Large Dataset**: Better coherence, domain knowledge
- **Extended Training**: Improved fluency and consistency

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ Thank's

Build on **LLama3** style architecture and techniques

## ğŸ™ Acknowledgments

- Inspired by the LLaMA architecture from Meta AI
- Uses SentencePiece tokenization
- Built with PyTorch framework
- Wikipedia for providing open data

## ğŸ“ Support

For questions and support:
- Open an issue on GitHub
- Check the troubleshooting section
- Review the code comments for implementation details
- Mail us hemanthreddykunduru0701@gmail.com

---
# Sequence Diagram
```mermaid
graph TB
    %% Data Collection Phase
    subgraph DC["ğŸ” PHASE 1: DATA COLLECTION"]
        DC1[DataCollector Class<br/>ğŸ“ output_dir='data']
        DC2[scrape_wikipedia<br/>ğŸŒ Wikipedia API<br/>ğŸ“Š max_articles=2000/topic<br/>ğŸ¯ Topic: 'artificial intelligence']
        DC3[scrape_web_content<br/>ğŸŒ Web URLs scraping<br/>ğŸ“„ BeautifulSoup parsing<br/>â±ï¸ timeout=10s]
        DC4[clean_text<br/>ğŸ§¹ Remove whitespace<br/>ğŸ”¤ Filter special chars<br/>ğŸ“ Min line length=10]
        DC5[save_raw_data<br/>ğŸ’¾ raw_data.json<br/>ğŸ“ˆ UTF-8 encoding]
        
        DC1 --> DC2
        DC1 --> DC3
        DC2 --> DC4
        DC3 --> DC4
        DC4 --> DC5
    end

    %% Data Preprocessing Phase
    subgraph DP["ğŸ”§ PHASE 2: DATA PREPROCESSING"]
        DP1[DataPreprocessor Class<br/>ğŸ“‚ data_dir='data']
        DP2[load_raw_data<br/>ğŸ“– Load raw_data.json]
        DP3[deduplicate_exact<br/>ğŸ” MD5 hashing<br/>ğŸ“Š Remove exact matches]
        DP4[deduplicate_near<br/>ğŸ“ Jaccard similarity<br/>ğŸ”¢ k-shingles=3<br/>âš–ï¸ threshold=0.7]
        DP5[quality_filter<br/>ğŸ“ Length: 100-50000 chars<br/>ğŸ“ Words: >20<br/>ğŸ“– Sentence structure<br/>ğŸ¨ Character diversity>10]
        DP6[save_processed_data<br/>ğŸ’¾ clean_data.json<br/>âœ… High-quality texts]
        
        DP1 --> DP2
        DP2 --> DP3
        DP3 --> DP4
        DP4 --> DP5
        DP5 --> DP6
    end

    %% Tokenization Phase
    subgraph TT["ğŸ”¤ PHASE 3: TOKENIZER TRAINING"]
        TT1[TokenizerTrainer Class<br/>ğŸ“Š vocab_size=12000]
        TT2[train_tokenizer<br/>ğŸ¤– SentencePiece BPE<br/>ğŸ”„ model_type='bpe'<br/>ğŸ“‹ normalization='nfkc']
        TT3[Special Tokens<br/>ğŸ”¹ PAD_ID=0<br/>ğŸ”¹ EOS_ID=1<br/>ğŸ”¹ UNK_ID=2<br/>ğŸ”¹ BOS_ID=3]
        TT4[tinyllama_tokenizer.model<br/>ğŸ’¾ Trained tokenizer<br/>ğŸ“Š Vocab size: 12K]
        
        TT1 --> TT2
        TT2 --> TT3
        TT3 --> TT4
    end

    %% Model Architecture Detail
    subgraph MA["ğŸ§  PHASE 4: MODEL ARCHITECTURE"]
        subgraph TME["Token & Position Embeddings"]
            TME1[Token Embedding<br/>ğŸ“Š vocab_size=12000<br/>â¡ï¸ dim=512<br/>ğŸ¯ Learnable weights]
        end
        
        subgraph TB["ğŸ”„ Transformer Block Ã—8"]
            TB1[Input: B,T,512]
            TB2[RMSNorm<br/>ğŸ“ Root Mean Square<br/>âš¡ eps=1e-6<br/>ğŸ›ï¸ Learnable scale]
            TB3[Multi-Head Attention<br/>ğŸ‘¥ n_heads=8<br/>ğŸ“ head_dim=64<br/>ğŸ”„ Q,K,V projections]
            TB4[Rotary Position Encoding<br/>ğŸŒ€ RoPE for Q,K<br/>ğŸ“ base=10000<br/>ğŸ“ max_pos=2048]
            TB5[Causal Attention<br/>ğŸ”º Lower triangular mask<br/>âš¡ Scaled dot-product<br/>ğŸ“Š Softmax normalization]
            TB6[Output Projection<br/>ğŸ”„ Linear transformation<br/>ğŸ“Š dimâ†’dim]
            TB7[Residual Connection 1<br/>â• x + attentionnorm_x]
            TB8[RMSNorm<br/>ğŸ“ FFN normalization]
            TB9[SwiGLU Feed Forward<br/>ğŸ”€ w1: dimâ†’4*dim<br/>ğŸ”€ w2: 4*dimâ†’dim<br/>ğŸ”€ w3: dimâ†’4*dim<br/>âš¡ SiLU w1_x * w3_x]
            TB10[Residual Connection 2<br/>â• x + ffn norm_x]
            TB11[Output: B,T,512]
            
            TB1 --> TB2
            TB2 --> TB3
            TB3 --> TB4
            TB4 --> TB5
            TB5 --> TB6
            TB6 --> TB7
            TB7 --> TB8
            TB8 --> TB9
            TB9 --> TB10
            TB10 --> TB11
        end
        
        subgraph FIN["ğŸ¯ Final Layers"]
            FIN1[Final RMSNorm<br/>ğŸ“ Layer normalization]
            FIN2[Language Model Head<br/>ğŸ”„ Linear: dimâ†’vocab_size<br/>ğŸ“Š No bias<br/>ğŸ¯ Logit computation]
            FIN3[Cross-Entropy Loss<br/>ğŸ“Š Next token prediction<br/>ğŸ¯ ignore_index=-1]
            
            FIN1 --> FIN2
            FIN2 --> FIN3
        end
        
        TME1 --> TB1
        TB11 --> FIN1
    end

    %% Dataset and Training
    subgraph DS["ğŸ“š DATASET PREPARATION"]
        DS1[TextDataset Class<br/>ğŸ“„ Text processing<br/>ğŸ”¤ Tokenization<br/>ğŸ“ max_length=512]
        DS2[Data Splitting<br/>ğŸ¯ Train: 90%<br/>âœ… Validation: 10%<br/>ğŸ”€ Shuffle enabled]
        DS3[DataLoader<br/>ğŸ“¦ batch_size=8<br/>ğŸ”€ shuffle=True<br/>âš¡ Efficient batching]
        DS4[Input Sequences<br/>ğŸ“Š x: tokens :-1<br/>ğŸ¯ y: tokens 1: <br/>ğŸ”„ Shifted targets]
        
        DS1 --> DS2
        DS2 --> DS3
        DS3 --> DS4
    end

    %% Training Pipeline
    subgraph TR["ğŸ‹ï¸ PHASE 5: TRAINING PIPELINE"]
        TR1[Trainer Class<br/>ğŸ–¥ï¸ Device: CUDA/CPU<br/>ğŸ“Š Model parameters: ~24M]
        TR2[AdamW Optimizer<br/>ğŸ“ˆ lr=5e-4<br/>âš–ï¸ weight_decay=0.1<br/>ğŸ¯ Beta parameters]
        TR3[Cosine Annealing LR<br/>ğŸ“‰ T_max=total_steps<br/>ğŸ”„ Smooth decay<br/>ğŸ“Š Min lr approaches 0]
        TR4[Training Loop<br/>ğŸ”„ epochs=100<br/>ğŸ’¾ save_every=1000<br/>ğŸ“Š Progress tracking]
        
        subgraph TS["ğŸ”„ Training Step Detail"]
            TS1[Forward Pass<br/>ğŸ“Š Compute logits<br/>ğŸ“ˆ Calculate loss<br/>ğŸ¯ Causal LM objective]
            TS2[Backward Pass<br/>ğŸ”„ loss.backward<br/>ğŸ“Š Gradient computation<br/>âš¡ Automatic differentiation]
            TS3[Gradient Clipping<br/>âœ‚ï¸ max_norm=1.0<br/>ğŸ›¡ï¸ Prevent explosion<br/>ğŸ“Š L2 norm clipping]
            TS4[Parameter Update<br/>ğŸ“ˆ optimizer.step<br/>ğŸ”„ Weight adjustment<br/>âš¡ AdamW mechanics]
            TS5[LR Scheduling<br/>ğŸ“‰ scheduler.step<br/>ğŸ”„ Learning rate decay<br/>ğŸ“Š Cosine annealing]
            TS6[Validation<br/>âœ… Eval mode<br/>ğŸ“Š No gradients<br/>ğŸ“ˆ Track val_loss]
            TS7[Checkpointing<br/>ğŸ’¾ Save model state<br/>ğŸ“‹ Save config<br/>ğŸ”„ Resume capability]
            
            TS1 --> TS2
            TS2 --> TS3
            TS3 --> TS4
            TS4 --> TS5
            TS5 --> TS6
            TS6 --> TS7
        end
        
        TR1 --> TR2
        TR2 --> TR3
        TR3 --> TR4
        TR4 --> TS1
    end

    %% Inference Pipeline
    subgraph INF["ğŸš€ PHASE 6: INFERENCE PIPELINE"]
        INF1[TinyLLaMAInference<br/>ğŸ“‹ Load checkpoint<br/>ğŸ”¤ Load tokenizer<br/>ğŸ–¥ï¸ Set device]
        INF2[Model Loading<br/>ğŸ“Š Restore state_dict<br/>âš™ï¸ Apply config<br/>ğŸ”„ Set eval mode]
        INF3[Text Generation<br/>ğŸ¯ Autoregressive<br/>ğŸ”„ One token at a time<br/>ğŸ“ Max sequence length]
        
        subgraph GEN["ğŸ¨ Generation Process"]
            GEN1[Input Processing<br/>ğŸ”¤ Tokenize prompt<br/>ğŸ“Š Convert to tensor<br/>ğŸ–¥ï¸ Move to device]
            GEN2[Forward Pass<br/>ğŸ“Š Model inference<br/>ğŸ¯ Get logits<br/>ğŸ“ˆ No gradients]
            GEN3[Temperature Scaling<br/>ğŸŒ¡ï¸ logits / temperature<br/>ğŸ¯ Control randomness<br/>ğŸ“Š Default: 0.7]
            GEN4[Top-k Filtering<br/>ğŸ” Keep top k tokens<br/>ğŸ“Š Default: k=50<br/>âœ‚ï¸ Mask others to -inf]
            GEN5[Sampling<br/>ğŸ² Multinomial sampling<br/>ğŸ“Š Probability distribution<br/>ğŸ”„ Random selection]
            GEN6[Token Append<br/>â• Add to sequence<br/>ğŸ”„ Continue generation<br/>ğŸ“ Check max length]
            GEN7[Stopping Criteria<br/>ğŸ›‘ Max length reached<br/>ğŸ”š EOS token found<br/>â¹ï¸ User termination]
            GEN8[Decoding<br/>ğŸ”¤ Tokens to text<br/>ğŸ“ SentencePiece decode<br/>âœ¨ Final output]
            
            GEN1 --> GEN2
            GEN2 --> GEN3
            GEN3 --> GEN4
            GEN4 --> GEN5
            GEN5 --> GEN6
            GEN6 --> GEN7
            GEN7 --> GEN8
            GEN6 -.->|Continue| GEN2
        end
        
        INF1 --> INF2
        INF2 --> INF3
        INF3 --> GEN1
    end

    %% Model Specifications
    subgraph SPECS["ğŸ“Š MODEL SPECIFICATIONS"]
        SPEC1[Architecture Details<br/>ğŸ§  Parameters: ~24M<br/>ğŸ“Š Vocabulary: 12,000<br/>ğŸ“ Hidden dim: 512<br/>ğŸ”„ Layers: 8<br/>ğŸ‘¥ Attention heads: 8<br/>ğŸ“ Max sequence: 512<br/>ğŸ¯ Context window: 512]
        
        SPEC2[Training Configuration<br/>ğŸ“š Data: AI Wikipedia<br/>ğŸ“„ Articles: ~2000<br/>ğŸ”„ Epochs: 100<br/>ğŸ“¦ Batch size: 8<br/>ğŸ“ˆ Learning rate: 5e-4<br/>âš–ï¸ Weight decay: 0.1<br/>ğŸ¯ Optimizer: AdamW]
        
        SPEC3[Performance Metrics<br/>ğŸ’¾ Memory: ~4GB GPU<br/>â±ï¸ Training time: Hours<br/>ğŸš€ Inference: Real-time<br/>ğŸ“Š Loss convergence<br/>âœ¨ Text quality<br/>ğŸ¯ Coherence score]
        
        SPEC4[Technical Features<br/>ğŸŒ€ Rotary embeddings<br/>ğŸ“ RMS normalization<br/>âš¡ SwiGLU activation<br/>ğŸ”º Causal attention<br/>âœ‚ï¸ Gradient clipping<br/>ğŸ“‰ LR scheduling]
    end

    %% Main Execution Flow
    subgraph MAIN["ğŸ¬ MAIN EXECUTION FLOW"]
        MAIN1[main Function<br/>ğŸ–¥ï¸ Device selection<br/>ğŸ¯ CUDA/CPU detection<br/>ğŸ“Š Memory check]
        MAIN2[Sequential Execution<br/>1ï¸âƒ£ Data collection<br/>2ï¸âƒ£ Preprocessing<br/>3ï¸âƒ£ Tokenizer training<br/>4ï¸âƒ£ Model training<br/>5ï¸âƒ£ Inference testing<br/>âœ… Complete pipeline]
        MAIN3[Output Artifacts<br/>ğŸ’¾ tinyllama_final.pt<br/>ğŸ”¤ tinyllama_tokenizer.model<br/>ğŸ“Š Training logs<br/>ğŸ’¹ Checkpoints<br/>ğŸ“‹ Configuration files]
        
        MAIN1 --> MAIN2
        MAIN2 --> MAIN3
    end

    %% Data Flow Connections
    DC5 -.->|raw_data.json| DP2
    DP6 -.->|clean_data.json| TT2
    TT4 -.->|tokenizer.model| DS1
    DS4 -.->|batched_data| TR4
    TR4 -.->|trained_model| INF1
    
    %% Control Flow
    MAIN1 --> DC1
    DC5 --> DP1
    DP6 --> TT1
    TT4 --> DS1
    DS4 --> TR1
    TR4 --> INF1

    %% Elegant Light Color Styling
    classDef dataCollection fill:#e8f4fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1
    classDef preprocessing fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#2e7d32
    classDef tokenization fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#ad1457
    classDef architecture fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#6a1b9a
    classDef training fill:#fff8e1,stroke:#ff8f00,stroke-width:2px,color:#e65100
    classDef inference fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#004d40
    classDef specs fill:#fafafa,stroke:#424242,stroke-width:2px,color:#212121
    classDef main fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px,color:#303f9f
    classDef process fill:#fff3e0,stroke:#f57c00,stroke-width:1px,color:#ef6c00
    classDef component fill:#f9fbe7,stroke:#827717,stroke-width:1px,color:#689f38

    %% Apply styles to phases
    class DC,DC1,DC2,DC3,DC4,DC5 dataCollection
    class DP,DP1,DP2,DP3,DP4,DP5,DP6 preprocessing
    class TT,TT1,TT2,TT3,TT4 tokenization
    class MA,TME,TME1,TB,TB1,TB2,TB3,TB4,TB5,TB6,TB7,TB8,TB9,TB10,TB11,FIN,FIN1,FIN2,FIN3 architecture
    class TR,TR1,TR2,TR3,TR4,DS,DS1,DS2,DS3,DS4 training
    class INF,INF1,INF2,INF3,GEN,GEN1,GEN2,GEN3,GEN4,GEN5,GEN6,GEN7,GEN8 inference
    class SPECS,SPEC1,SPEC2,SPEC3,SPEC4 specs
    class MAIN,MAIN1,MAIN2,MAIN3 main
    class TS,TS1,TS2,TS3,TS4,TS5,TS6,TS7 process
```
