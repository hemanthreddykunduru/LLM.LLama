graph TB
    %% Data Collection Phase
    subgraph DC["ğŸ” PHASE 1: DATA COLLECTION"]
        DC1[DataCollector Class<br/>ğŸ“ output_dir='data']
        DC2[scrape_wikipedia<br/>ğŸŒ Wikipedia API<br/>ğŸ“Š max_articles=2000/topic<br/>ğŸ¯ Topic: 'artificial intelligence']
        DC3[scrape_web_content<br/>ğŸŒ Web URLs scraping<br/>ğŸ“„ BeautifulSoup parsing<br/>â±ï¸ timeout=10s]
        DC4[clean_text<br/>ğŸ§¹ Remove whitespace<br/>ğŸ”¤ Filter special chars<br/>ğŸ“ Min line length=10]
        DC5[save_raw_data<br/>ğŸ’¾ raw_data.json<br/>ğŸ“ˆ UTF-8 encoding]
        
        DC1 --> DC2
        DC1 --> DC3
        DC2 --> DC4
        DC3 --> DC4
        DC4 --> DC5
    end

    %% Data Preprocessing Phase
    subgraph DP["ğŸ”§ PHASE 2: DATA PREPROCESSING"]
        DP1[DataPreprocessor Class<br/>ğŸ“‚ data_dir='data']
        DP2[load_raw_data<br/>ğŸ“– Load raw_data.json]
        DP3[deduplicate_exact<br/>ğŸ” MD5 hashing<br/>ğŸ“Š Remove exact matches]
        DP4[deduplicate_near<br/>ğŸ“ Jaccard similarity<br/>ğŸ”¢ k-shingles=3<br/>âš–ï¸ threshold=0.7]
        DP5[quality_filter<br/>ğŸ“ Length: 100-50000 chars<br/>ğŸ“ Words: >20<br/>ğŸ“– Sentence structure<br/>ğŸ¨ Character diversity>10]
        DP6[save_processed_data<br/>ğŸ’¾ clean_data.json<br/>âœ… High-quality texts]
        
        DP1 --> DP2
        DP2 --> DP3
        DP3 --> DP4
        DP4 --> DP5
        DP5 --> DP6
    end

    %% Tokenization Phase
    subgraph TT["ğŸ”¤ PHASE 3: TOKENIZER TRAINING"]
        TT1[TokenizerTrainer Class<br/>ğŸ“Š vocab_size=12000]
        TT2[train_tokenizer<br/>ğŸ¤– SentencePiece BPE<br/>ğŸ”„ model_type='bpe'<br/>ğŸ“‹ normalization='nfkc']
        TT3[Special Tokens<br/>ğŸ”¹ PAD_ID=0<br/>ğŸ”¹ EOS_ID=1<br/>ğŸ”¹ UNK_ID=2<br/>ğŸ”¹ BOS_ID=3]
        TT4[tinyllama_tokenizer.model<br/>ğŸ’¾ Trained tokenizer<br/>ğŸ“Š Vocab size: 12K]
        
        TT1 --> TT2
        TT2 --> TT3
        TT3 --> TT4
    end

    %% Model Architecture Detail
    subgraph MA["ğŸ§  PHASE 4: MODEL ARCHITECTURE"]
        subgraph TME["Token & Position Embeddings"]
            TME1[Token Embedding<br/>ğŸ“Š vocab_size=12000<br/>â¡ï¸ dim=512<br/>ğŸ¯ Learnable weights]
        end
        
        subgraph TB["ğŸ”„ Transformer Block Ã—8"]
            TB1[Input: B,T,512]
            TB2[RMSNorm<br/>ğŸ“ Root Mean Square<br/>âš¡ eps=1e-6<br/>ğŸ›ï¸ Learnable scale]
            TB3[Multi-Head Attention<br/>ğŸ‘¥ n_heads=8<br/>ğŸ“ head_dim=64<br/>ğŸ”„ Q,K,V projections]
            TB4[Rotary Position Encoding<br/>ğŸŒ€ RoPE for Q,K<br/>ğŸ“ base=10000<br/>ğŸ“ max_pos=2048]
            TB5[Causal Attention<br/>ğŸ”º Lower triangular mask<br/>âš¡ Scaled dot-product<br/>ğŸ“Š Softmax normalization]
            TB6[Output Projection<br/>ğŸ”„ Linear transformation<br/>ğŸ“Š dimâ†’dim]
            TB7[Residual Connection 1<br/>â• x + attentionnorm_x]
            TB8[RMSNorm<br/>ğŸ“ FFN normalization]
            TB9[SwiGLU Feed Forward<br/>ğŸ”€ w1: dimâ†’4*dim<br/>ğŸ”€ w2: 4*dimâ†’dim<br/>ğŸ”€ w3: dimâ†’4*dim<br/>âš¡ SiLU w1_x * w3_x]
            TB10[Residual Connection 2<br/>â• x + ffn norm_x]
            TB11[Output: B,T,512]
            
            TB1 --> TB2
            TB2 --> TB3
            TB3 --> TB4
            TB4 --> TB5
            TB5 --> TB6
            TB6 --> TB7
            TB7 --> TB8
            TB8 --> TB9
            TB9 --> TB10
            TB10 --> TB11
        end
        
        subgraph FIN["ğŸ¯ Final Layers"]
            FIN1[Final RMSNorm<br/>ğŸ“ Layer normalization]
            FIN2[Language Model Head<br/>ğŸ”„ Linear: dimâ†’vocab_size<br/>ğŸ“Š No bias<br/>ğŸ¯ Logit computation]
            FIN3[Cross-Entropy Loss<br/>ğŸ“Š Next token prediction<br/>ğŸ¯ ignore_index=-1]
            
            FIN1 --> FIN2
            FIN2 --> FIN3
        end
        
        TME1 --> TB1
        TB11 --> FIN1
    end

    %% Dataset and Training
    subgraph DS["ğŸ“š DATASET PREPARATION"]
        DS1[TextDataset Class<br/>ğŸ“„ Text processing<br/>ğŸ”¤ Tokenization<br/>ğŸ“ max_length=512]
        DS2[Data Splitting<br/>ğŸ¯ Train: 90%<br/>âœ… Validation: 10%<br/>ğŸ”€ Shuffle enabled]
        DS3[DataLoader<br/>ğŸ“¦ batch_size=8<br/>ğŸ”€ shuffle=True<br/>âš¡ Efficient batching]
        DS4[Input Sequences<br/>ğŸ“Š x: tokens :-1<br/>ğŸ¯ y: tokens 1: <br/>ğŸ”„ Shifted targets]
        
        DS1 --> DS2
        DS2 --> DS3
        DS3 --> DS4
    end

    %% Training Pipeline
    subgraph TR["ğŸ‹ï¸ PHASE 5: TRAINING PIPELINE"]
        TR1[Trainer Class<br/>ğŸ–¥ï¸ Device: CUDA/CPU<br/>ğŸ“Š Model parameters: ~24M]
        TR2[AdamW Optimizer<br/>ğŸ“ˆ lr=5e-4<br/>âš–ï¸ weight_decay=0.1<br/>ğŸ¯ Beta parameters]
        TR3[Cosine Annealing LR<br/>ğŸ“‰ T_max=total_steps<br/>ğŸ”„ Smooth decay<br/>ğŸ“Š Min lr approaches 0]
        TR4[Training Loop<br/>ğŸ”„ epochs=100<br/>ğŸ’¾ save_every=1000<br/>ğŸ“Š Progress tracking]
        
        subgraph TS["ğŸ”„ Training Step Detail"]
            TS1[Forward Pass<br/>ğŸ“Š Compute logits<br/>ğŸ“ˆ Calculate loss<br/>ğŸ¯ Causal LM objective]
            TS2[Backward Pass<br/>ğŸ”„ loss.backward<br/>ğŸ“Š Gradient computation<br/>âš¡ Automatic differentiation]
            TS3[Gradient Clipping<br/>âœ‚ï¸ max_norm=1.0<br/>ğŸ›¡ï¸ Prevent explosion<br/>ğŸ“Š L2 norm clipping]
            TS4[Parameter Update<br/>ğŸ“ˆ optimizer.step<br/>ğŸ”„ Weight adjustment<br/>âš¡ AdamW mechanics]
            TS5[LR Scheduling<br/>ğŸ“‰ scheduler.step<br/>ğŸ”„ Learning rate decay<br/>ğŸ“Š Cosine annealing]
            TS6[Validation<br/>âœ… Eval mode<br/>ğŸ“Š No gradients<br/>ğŸ“ˆ Track val_loss]
            TS7[Checkpointing<br/>ğŸ’¾ Save model state<br/>ğŸ“‹ Save config<br/>ğŸ”„ Resume capability]
            
            TS1 --> TS2
            TS2 --> TS3
            TS3 --> TS4
            TS4 --> TS5
            TS5 --> TS6
            TS6 --> TS7
        end
        
        TR1 --> TR2
        TR2 --> TR3
        TR3 --> TR4
        TR4 --> TS1
    end

    %% Inference Pipeline
    subgraph INF["ğŸš€ PHASE 6: INFERENCE PIPELINE"]
        INF1[TinyLLaMAInference<br/>ğŸ“‹ Load checkpoint<br/>ğŸ”¤ Load tokenizer<br/>ğŸ–¥ï¸ Set device]
        INF2[Model Loading<br/>ğŸ“Š Restore state_dict<br/>âš™ï¸ Apply config<br/>ğŸ”„ Set eval mode]
        INF3[Text Generation<br/>ğŸ¯ Autoregressive<br/>ğŸ”„ One token at a time<br/>ğŸ“ Max sequence length]
        
        subgraph GEN["ğŸ¨ Generation Process"]
            GEN1[Input Processing<br/>ğŸ”¤ Tokenize prompt<br/>ğŸ“Š Convert to tensor<br/>ğŸ–¥ï¸ Move to device]
            GEN2[Forward Pass<br/>ğŸ“Š Model inference<br/>ğŸ¯ Get logits<br/>ğŸ“ˆ No gradients]
            GEN3[Temperature Scaling<br/>ğŸŒ¡ï¸ logits / temperature<br/>ğŸ¯ Control randomness<br/>ğŸ“Š Default: 0.7]
            GEN4[Top-k Filtering<br/>ğŸ” Keep top k tokens<br/>ğŸ“Š Default: k=50<br/>âœ‚ï¸ Mask others to -inf]
            GEN5[Sampling<br/>ğŸ² Multinomial sampling<br/>ğŸ“Š Probability distribution<br/>ğŸ”„ Random selection]
            GEN6[Token Append<br/>â• Add to sequence<br/>ğŸ”„ Continue generation<br/>ğŸ“ Check max length]
            GEN7[Stopping Criteria<br/>ğŸ›‘ Max length reached<br/>ğŸ”š EOS token found<br/>â¹ï¸ User termination]
            GEN8[Decoding<br/>ğŸ”¤ Tokens to text<br/>ğŸ“ SentencePiece decode<br/>âœ¨ Final output]
            
            GEN1 --> GEN2
            GEN2 --> GEN3
            GEN3 --> GEN4
            GEN4 --> GEN5
            GEN5 --> GEN6
            GEN6 --> GEN7
            GEN7 --> GEN8
            GEN6 -.->|Continue| GEN2
        end
        
        INF1 --> INF2
        INF2 --> INF3
        INF3 --> GEN1
    end

    %% Model Specifications
    subgraph SPECS["ğŸ“Š MODEL SPECIFICATIONS"]
        SPEC1[Architecture Details<br/>ğŸ§  Parameters: ~24M<br/>ğŸ“Š Vocabulary: 12,000<br/>ğŸ“ Hidden dim: 512<br/>ğŸ”„ Layers: 8<br/>ğŸ‘¥ Attention heads: 8<br/>ğŸ“ Max sequence: 512<br/>ğŸ¯ Context window: 512]
        
        SPEC2[Training Configuration<br/>ğŸ“š Data: AI Wikipedia<br/>ğŸ“„ Articles: ~2000<br/>ğŸ”„ Epochs: 100<br/>ğŸ“¦ Batch size: 8<br/>ğŸ“ˆ Learning rate: 5e-4<br/>âš–ï¸ Weight decay: 0.1<br/>ğŸ¯ Optimizer: AdamW]
        
        SPEC3[Performance Metrics<br/>ğŸ’¾ Memory: ~4GB GPU<br/>â±ï¸ Training time: Hours<br/>ğŸš€ Inference: Real-time<br/>ğŸ“Š Loss convergence<br/>âœ¨ Text quality<br/>ğŸ¯ Coherence score]
        
        SPEC4[Technical Features<br/>ğŸŒ€ Rotary embeddings<br/>ğŸ“ RMS normalization<br/>âš¡ SwiGLU activation<br/>ğŸ”º Causal attention<br/>âœ‚ï¸ Gradient clipping<br/>ğŸ“‰ LR scheduling]
    end

    %% Main Execution Flow
    subgraph MAIN["ğŸ¬ MAIN EXECUTION FLOW"]
        MAIN1[main Function<br/>ğŸ–¥ï¸ Device selection<br/>ğŸ¯ CUDA/CPU detection<br/>ğŸ“Š Memory check]
        MAIN2[Sequential Execution<br/>1ï¸âƒ£ Data collection<br/>2ï¸âƒ£ Preprocessing<br/>3ï¸âƒ£ Tokenizer training<br/>4ï¸âƒ£ Model training<br/>5ï¸âƒ£ Inference testing<br/>âœ… Complete pipeline]
        MAIN3[Output Artifacts<br/>ğŸ’¾ tinyllama_final.pt<br/>ğŸ”¤ tinyllama_tokenizer.model<br/>ğŸ“Š Training logs<br/>ğŸ’¹ Checkpoints<br/>ğŸ“‹ Configuration files]
        
        MAIN1 --> MAIN2
        MAIN2 --> MAIN3
    end

    %% Data Flow Connections
    DC5 -.->|raw_data.json| DP2
    DP6 -.->|clean_data.json| TT2
    TT4 -.->|tokenizer.model| DS1
    DS4 -.->|batched_data| TR4
    TR4 -.->|trained_model| INF1
    
    %% Control Flow
    MAIN1 --> DC1
    DC5 --> DP1
    DP6 --> TT1
    TT4 --> DS1
    DS4 --> TR1
    TR4 --> INF1

    %% Elegant Light Color Styling
    classDef dataCollection fill:#e8f4fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1
    classDef preprocessing fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#2e7d32
    classDef tokenization fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#ad1457
    classDef architecture fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#6a1b9a
    classDef training fill:#fff8e1,stroke:#ff8f00,stroke-width:2px,color:#e65100
    classDef inference fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#004d40
    classDef specs fill:#fafafa,stroke:#424242,stroke-width:2px,color:#212121
    classDef main fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px,color:#303f9f
    classDef process fill:#fff3e0,stroke:#f57c00,stroke-width:1px,color:#ef6c00
    classDef component fill:#f9fbe7,stroke:#827717,stroke-width:1px,color:#689f38

    %% Apply styles to phases
    class DC,DC1,DC2,DC3,DC4,DC5 dataCollection
    class DP,DP1,DP2,DP3,DP4,DP5,DP6 preprocessing
    class TT,TT1,TT2,TT3,TT4 tokenization
    class MA,TME,TME1,TB,TB1,TB2,TB3,TB4,TB5,TB6,TB7,TB8,TB9,TB10,TB11,FIN,FIN1,FIN2,FIN3 architecture
    class TR,TR1,TR2,TR3,TR4,DS,DS1,DS2,DS3,DS4 training
    class INF,INF1,INF2,INF3,GEN,GEN1,GEN2,GEN3,GEN4,GEN5,GEN6,GEN7,GEN8 inference
    class SPECS,SPEC1,SPEC2,SPEC3,SPEC4 specs
    class MAIN,MAIN1,MAIN2,MAIN3 main
    class TS,TS1,TS2,TS3,TS4,TS5,TS6,TS7 process